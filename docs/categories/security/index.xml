<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Security on Mike&#39;s Blog</title>
    <link>https://michael-myers.github.io/blog/categories/security/index.xml</link>
    <description>Recent content in Security on Mike&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://michael-myers.github.io/blog/categories/security/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Look at the Rust Programming Language</title>
      <link>https://michael-myers.github.io/blog/post/a-tour-of-rust/</link>
      <pubDate>Mon, 27 Feb 2017 13:06:16 -0500</pubDate>
      
      <guid>https://michael-myers.github.io/blog/post/a-tour-of-rust/</guid>
      <description>

&lt;h2 id=&#34;where-to-find-more-execution-performance&#34;&gt;Where to Find More Execution Performance&lt;/h2&gt;

&lt;p&gt;Moore&amp;rsquo;s Law &lt;a href=&#34;http://fortune.com/2017/01/05/intel-ces-2017-moore-law/&#34;&gt;is just about done&lt;/a&gt;. It once described a trend of transistor count doubling every 24 months (enabled by increasing the density of transistors by making them ever-smaller). Now:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Between the introduction of 65 nm and 45 nm chips, about 23 months passed. To get from 45 nm to 32 nm took about 27 months, 28 months to go down from there to 22 nm and 30 months to shrink to the current 14 nm process. And that&amp;rsquo;s where Intel has been stuck since September 2014.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Intel &lt;em&gt;might&lt;/em&gt; release 10nm scale chips in late 2017, which would mean that they worked 36-40 months in order to shrink from 14nm to 10nm scale. In other words, the most recent density doubling (the shrink from 22nm to 10nm), by the time it happens, will have taken over 5 years. The next doubling is likely to take at least that long, assuming the multiple breakthroughs required to do so can even be achieved. 10nm is already fairly close to the atomic scale: ~45 silicon &lt;em&gt;atoms&lt;/em&gt; across (one atom: 0.22nm). One of the obstacles at this scale to be addressed is &lt;a href=&#34;https://en.wikipedia.org/wiki/Quantum_tunnelling&#34;&gt;quantum tunneling&lt;/a&gt;, not that I pretend to understand it.&lt;/p&gt;

&lt;p&gt;Of course, Moore&amp;rsquo;s Law can be satisfied one other way without changing density, which is to simply use bigger and bigger processor dies. You may have seen charts showing that transistor &lt;em&gt;count&lt;/em&gt; continues to increase on schedule with Moore&amp;rsquo;s Law, but this is only true for dedicated GPUs and high-end server CPUs, which are already up against cost practicality limits due to these die sizes.&lt;/p&gt;

&lt;p&gt;Even if we were still on track for Moore&amp;rsquo;s Law, increasing transistor counts alone have &lt;a href=&#34;https://cartesianproduct.wordpress.com/2013/04/15/the-end-of-dennard-scaling/&#34;&gt;provided diminishing returns as of late&lt;/a&gt;. Recent density increases have mainly just served to reduce power draw and to make more space on the CPU die dedicated to graphics rendering (an ideal parallelizable task). Tech being an optimistic culture makes it slow to acknowledge the obvious truth here: CPU &lt;em&gt;cores&lt;/em&gt; aren&amp;rsquo;t getting significantly faster. Unless your work is on a mobile device or can be delegated to a GPU or server farm, your only performance upgrades since 2010 have been I/O-related ones.&lt;/p&gt;

&lt;p&gt;Granted, transistor density improvements have continued to increase CPU power efficiency. But I have a Intel &amp;ldquo;Core i7&amp;rdquo; (2.66 GHz i7-620M, 2-core) laptop that will turn 7 years old in a couple of months, and today&amp;rsquo;s equivalent CPUs &lt;em&gt;still&lt;/em&gt; offer only a marginal performance improvement for tasks that aren&amp;rsquo;t 3D graphics. The equivalent CPU today, the Intel &amp;ldquo;Core i7&amp;rdquo; (2.7GHz i7-7500U, 2-core), has single-threaded performance only about 60% better than my CPU from 7 years ago. Not enough to make me throw out my old laptop.&lt;/p&gt;

&lt;p&gt;All of this background is to make my point, which is that the next performance leap has to come from improved software, rather than relying on &amp;ldquo;free&amp;rdquo; improvements from new hardware. A few software methods for achieving a generational improvement in performance might be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parallelism&lt;/li&gt;
&lt;li&gt;Optimizing compilers&lt;/li&gt;
&lt;li&gt;Moving tasks from interpreted languages back to compiled languages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these things are already happening, but it&amp;rsquo;s the last one that I&amp;rsquo;m interested in most.&lt;/p&gt;

&lt;h2 id=&#34;parallelism&#34;&gt;Parallelism&lt;/h2&gt;

&lt;p&gt;Parallelism has brought great performance improvements in graphics, &amp;ldquo;AI,&amp;rdquo; and large data set processing (so-called &amp;ldquo;Big Data&amp;rdquo;), and is the reason why GPUs &lt;a href=&#34;https://en.wikipedia.org/wiki/Transistor_count#GPUs&#34;&gt;continue to march forward in transistor count&lt;/a&gt; (although, again, check out those increasing die sizes; those are approaching their own limits of practicality). The problem with parallelism, though, is that while there are some workloads that are naturally suited to it, others aren&amp;rsquo;t and never will be. Sometimes, computing Task B is dependent on the outcome of Task A, and there is just no way to split up Task A. Even when parts of a task &lt;em&gt;can&lt;/em&gt; be parallelized, there are swiftly diminishing returns to adding more cores, as described &lt;a href=&#34;https://en.wikipedia.org/wiki/Amdahl%27s_law&#34;&gt;by Amdahl&amp;rsquo;s Law&lt;/a&gt;. What parallelized processing &lt;em&gt;does&lt;/em&gt; scale well for &lt;a href=&#34;https://en.wikipedia.org/wiki/Gustafson%27s_law&#34;&gt;is large data sets&lt;/a&gt;, although the home user is not typically handling large data sets, and won&amp;rsquo;t directly benefit from this kind of parallelism.&lt;/p&gt;

&lt;h2 id=&#34;optimizing-compilers&#34;&gt;Optimizing Compilers&lt;/h2&gt;

&lt;p&gt;Here are &lt;a href=&#34;https://cr.yp.to/talks/2015.04.16/slides-djb-20150416-a4.pdf&#34;&gt;Daniel J Bernstein&amp;rsquo;s 2015 slides&lt;/a&gt; about the death of &amp;ldquo;optimizing compilers,&amp;rdquo; or rather, that despite all the hype about them, we are still manually tuning the performance critical portions of our programs. The optimizing compilers&amp;rsquo; optimization of non-critical code portions is irrelevant, or at least not worth the effort put into optimizing compilers. It appears that a compiler to generically optimize any code as well as an expert human could, would require something like a &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_general_intelligence&#34;&gt;general AI&lt;/a&gt; with a full contextual understanding of the problem being solved by the code. Such a thing doesn&amp;rsquo;t exist, and is not on the horizon.&lt;/p&gt;

&lt;h2 id=&#34;better-safer-compiled-languages&#34;&gt;Better (Safer) Compiled Languages&lt;/h2&gt;

&lt;p&gt;C and C++ never really left us, and neither have all of the inherent memory errors in code programmed in C and C++. That includes Java, whose runtime is still written in C. The Java runtime has been the source of many &amp;ldquo;Java&amp;rdquo; security issues over the years, to the point where the Java plug-in was effectively banned from all web browsers. Despite that, the rest of the browser is also written in C and C++, and just as prone to these problems. There hasn&amp;rsquo;t been any viable alternative but to try to sandbox and privilege-reduce the browser, because any safer language is too slow.&lt;/p&gt;

&lt;p&gt;The real cost of C and C++ &amp;rsquo;s performance is their high maintenance burdens: coding in them means always opening up subtle concurrency errors, memory corruption bugs, and information leak vulnerabilities. This is why simply improving the C++ standard library and adding more and more features to the language has not altered its basic value proposition to developers, who have already fled to &amp;ldquo;safe&amp;rdquo; languages.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s where the experimental language, Rust, comes in. It&amp;rsquo;s a compiled systems programming language with performance on par with (&lt;a href=&#34;http://benchmarksgame.alioth.debian.org/u64q/which-programs-are-fastest.html&#34;&gt;or better than&lt;/a&gt;) C++, but with compile-time restrictions on memory management and concurrency that should prevent entire classes of bugs. At some point in the next 5 years, I predict that we will see Rust (or something like it, whether it&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Swift_(programming_language)&#34;&gt;Swift&lt;/a&gt; or some new really strict C++ compiler) slowly start replacing C/C++ wherever performance and security are both primary concerns. It&amp;rsquo;s exciting to think that a well-designed compiled language could solve most of the reasons for the ~20-year flight away from native code programming.&lt;/p&gt;

&lt;p&gt;Having played with Rust for a few days, I can say it will certainly not replace Python for &lt;em&gt;ease&lt;/em&gt; of development, but it&amp;rsquo;s a really interesting disruptor for anyone writing native code. Security researchers should also take notice.&lt;/p&gt;

&lt;h2 id=&#34;rust-programming-language&#34;&gt;Rust Programming Language&lt;/h2&gt;

&lt;p&gt;For what it&amp;rsquo;s worth, Rust was the ‚ÄúMost Loved Programming Language of 2016 in the Stack Overflow Developer Survey.‚Äù It enforces memory management and safety at compile-time. Some memory safety features of the language include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Rust does not permit null pointers or dangling pointers. Since pointers are never NULL, you can always safely dereference a pointer.&lt;/li&gt;
&lt;li&gt;There are no ‚Äúvoid‚Äù pointers.&lt;/li&gt;
&lt;li&gt;Pointers can not be downcast to a more specific type, only upcast to a more generic type. If generic data structures are needed, you use parameterized types/functions.&lt;/li&gt;
&lt;li&gt;Variables can be allocated on the heap and are cleaned up without the need for ‚Äúfree‚Äù or ‚Äúdelete.‚Äù&lt;/li&gt;
&lt;li&gt;There can be only one pointer pointing to an allocation, and it is passed back and forth between ‚Äúowners‚Äù such that concurrent access race conditions are impossible. (*EDIT: &lt;a href=&#34;https://twitter.com/vitiral&#34;&gt;@vitiral&lt;/a&gt; clarifies that while there can only be one pointer if it&amp;rsquo;s &lt;strong&gt;mutable&lt;/strong&gt;, there can be infinite copies of an immuatable reference; however there cannot be both mutable and immutable references to the same allocation*)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you just wanted a statically typed, compiled language with a modern standard library that is easy to extend, you could also choose Go. But Rust claims to be all of that, plus faster and safer. Rust will work in embedded devices and other spaces currently occupied by C/C++; Go will not. &lt;a href=&#34;http://yager.io/programming/go.html&#34;&gt;Some think Rust is just fundamentally better&lt;/a&gt;, but I am not qualified to judge that.&lt;/p&gt;

&lt;h3 id=&#34;rust-and-parallelism&#34;&gt;Rust and parallelism&lt;/h3&gt;

&lt;p&gt;Rust makes parallelization an integral part of the language, with support for all of the necessary parallel programming primitives. Parallelized versions of various programming constructs can be swapped in without changing your existing code. This is possible because the Rust language forces the programmer to specify more about how data will be used, which prevents race conditions at runtime by turning them into errors at compile time, instead.&lt;/p&gt;

&lt;h3 id=&#34;concept-of-ownership-in-rust&#34;&gt;Concept of &amp;ldquo;Ownership&amp;rdquo; in Rust&lt;/h3&gt;

&lt;p&gt;The major innovation of the Rust language (inspired by a prior language, &amp;ldquo;Cyclone&amp;rdquo;) is that its compiler, in order to do memory management and prevent race conditions at compile time, tracks &amp;ldquo;ownership&amp;rdquo; of all variables in the code. Once a variable is used (like in a call to a function) it is considered to be passed to a new &amp;ldquo;owner,&amp;rdquo; and using it in a subsequent statement is illegal and would trigger a compiler error. If the developer&amp;rsquo;s intention was to copy-on-use (&amp;ldquo;clone&amp;rdquo;), they must specify that in their code. For certain simple data types (integers, etc.), they are automatically copied-on-use without any explicit intent from the developer. Another aspect of ownership in Rust is that all variables are (what in C/C++ would be called) &lt;code&gt;const&lt;/code&gt;, by default. In Rust, if you want a variable to be mutable, it has to be explicitly stated in the declaration.&lt;/p&gt;

&lt;p&gt;This concept is the foundation of the Rust language. It&amp;rsquo;s hard to grasp at first, since it is very different from programming in C or C++, or even Java. The most detailed explanation of Rust ownership that I&amp;rsquo;ve seen is &lt;a href=&#34;https://chrismorgan.info/blog/rust-ownership-the-hard-way.html&#34;&gt;this article by Chris Morgan&lt;/a&gt;, but to actually learn the concept I&amp;rsquo;d recommend starting with &lt;a href=&#34;http://intorust.com/tutorial/ownership/&#34;&gt;this 25 minute video by Nikolas Matsakis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At first, it seems like another mental burden on the programmer, but adopting this concept of memory management means the programmer is also &lt;em&gt;relieved&lt;/em&gt; of having to manage memory with carefully paired calls to &lt;code&gt;malloc()&lt;/code&gt; and &lt;code&gt;free()&lt;/code&gt; (or &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt;). &amp;ldquo;So what, isn&amp;rsquo;t this what you get with C# or Java?&amp;rdquo; Not quite: those languages use a Garbage Collector to track references to data at &lt;em&gt;runtime&lt;/em&gt;, which has an inherent performance overhead and whose &amp;ldquo;stop-the-world&amp;rdquo; resource management can be &lt;a href=&#34;http://stackoverflow.com/questions/16695874/why-does-the-jvm-full-gc-need-to-stop-the-world&#34;&gt;inconsistent and unpredictable&lt;/a&gt;. Rust does it in the language, at compile time. &lt;em&gt;So, without the use of a Garbage Collector, Rust makes memory management (and concurrent access to data) safe again.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;rust-is-a-drop-in-replacement-for-c&#34;&gt;Rust is a Drop-In Replacement for C&lt;/h3&gt;

&lt;p&gt;Just like C/C++, Rust can &lt;a href=&#34;https://blog.sentry.io/2016/10/19/fixing-python-performance-with-rust&#34;&gt;be coupled to Python or any other language with a native interface, in order to leverage the strengths of both&lt;/a&gt;. And, debugging Rust programs is officially &lt;a href=&#34;https://www.mail-archive.com/info-gnu@gnu.org/msg02192.html&#34;&gt;supported by GDB&lt;/a&gt;. This works the other way around too, i.e., you can build a Rust program on top of native code libraries written in C/C++. Mozilla is even working on &lt;a href=&#34;https://servo.org&#34;&gt;a web browser engine in Rust&lt;/a&gt;, to replace Gecko, the Firefox engine. &lt;a href=&#34;https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=MTgzNDA&#34;&gt;Benchmarks in 2014&lt;/a&gt; showed a 300% increase in performance vs Gecko, and by early 2016, it was &lt;a href=&#34;https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=Google-Servo-Perf-Comparison&#34;&gt;beating Webkit and Chrome as well&lt;/a&gt; (at least in some hand-picked benchmarks where they leverage Rust&amp;rsquo;s ease of parallelism to delegate a bunch of stuff to the GPU). If you&amp;rsquo;re interested in the details of how Rust can improve browser engines, Mozilla &lt;a href=&#34;https://arxiv.org/pdf/1505.07383v1.pdf&#34;&gt;wrote about it here&lt;/a&gt;. Buried in the paper is a detail that they seem to have downplayed elsewhere, though: the new browser engine is actually still bootstrapped by an existing codebase, so it&amp;rsquo;s still 75% C/C++ code. On the other hand, that also goes to show how Rust integrates well with C/C++.&lt;/p&gt;

&lt;h3 id=&#34;rust-has-a-package-manager-which-is-also-its-build-tool&#34;&gt;Rust has a Package Manager, which is also its Build Tool&lt;/h3&gt;

&lt;p&gt;Makefiles are impossible to write and debug, and basically you&amp;rsquo;re always just copy-pasting a previous Makefile into the new one, or hoping an IDE or build tool abstracts away all that crap for you, which is why &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_build_automation_software&#34;&gt;this wheel has been reinvented many times&lt;/a&gt;. I generally don&amp;rsquo;t have a favorite build tool (they&amp;rsquo;re all bad), since it always seems to come down to a manual troubleshooting cycle of acquiring all the right dependencies. The worst is having a build system that is a big layer cake of scripts on top of XML on top of Makefiles.&lt;/p&gt;

&lt;p&gt;Rust package manager &amp;ldquo;Cargo&amp;rdquo; simply uses TOML files to describe what a Rust project needs in order to build, and when you build with Cargo, it just goes out and gets those dependencies for you. Plus, the packages are served from Crates.io, so if you&amp;rsquo;re keeping score that&amp;rsquo;s a double tech hipster bonus for using both the .io domain &lt;em&gt;and&lt;/em&gt; TOML.&lt;/p&gt;

&lt;h2 id=&#34;installation-and-hello-world&#34;&gt;Installation and Hello World&lt;/h2&gt;

&lt;p&gt;Assuming you&amp;rsquo;re using MacOS like me (there is plenty of info out there already for Windows and Linux users) and you have Homebrew:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;    $ brew install rust
    $ rustc --version
    rustc 1.15.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You probably want an editor with Rust syntax highlighting and code completion. &lt;a href=&#34;https://areweideyet.com&#34;&gt;These are your choices&lt;/a&gt;. I went with Visual Studio Code, aka VS Code. It&amp;rsquo;s not what I&amp;rsquo;d call an IDE, and I still haven&amp;rsquo;t gotten it to integrate with a debugger, but hopefully JetBrains will step up and make a Rust IDE ‚Äì once there is a market for it.&lt;/p&gt;

&lt;p&gt;VS Code doesn&amp;rsquo;t understand Rust out of the box. Launching VS Code, hit Command-P to open the in-app console:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ext install vscode-rust
(install the top search result, should be the extension by kalitaalexey)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Optionally, you can install a GDB/LLDB integration layer to attempt to debug from VS Code (in theory ‚Äì YMMV but I haven&amp;rsquo;t gotten it to work for LLDB with C++ yet, let alone Rust):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ext install webfreak.debug
(install the top search result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice in the bottom right: ‚ÄúRust tools are missing‚Äù ‚Ä¶ click install. It will invoke Cargo (the Rust package manager) to download, compile, and install more of the Rust toolchain for you: racer, rustfmt, rustsym, etc. And all of the dependencies for those. Go have a coffee, this will take a while. About 18 minutes on my system.&lt;/p&gt;

&lt;p&gt;Finally: close VS Code, and open up Terminal so we can put all these new Rust binaries on your $PATH.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ open -a /Applications/TextEdit.app ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the line &lt;code&gt;export PATH=&amp;quot;/Users/yourusername/.cargo/bin:$PATH&amp;quot;&lt;/code&gt; and save.&lt;/p&gt;

&lt;p&gt;Open a new instance of VS Code. It should no longer tell you that Rust tools are missing. üëçüèª&lt;/p&gt;

&lt;p&gt;Test the environment with a Hello World in Rust! Save the following as hello.rs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn main() {
    println!(&amp;quot;Hello World!&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open &amp;ldquo;View -&amp;gt; Integrated Terminal.&amp;rdquo; From here you can compile by hand like a peasant, because VS Code isn‚Äôt an actual IDE.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash-3.2$ cd ~/Desktop
bash-3.2$ rustc hello.rs
bash-3.2$ ./hello
Hello World!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But for a realistic scenario, we could have also used Cargo to both create a new Rust project and then build it.&lt;/p&gt;

&lt;p&gt;In a future post, I will share my thoughts on what it&amp;rsquo;s like to try to actually write a program in Rust.&lt;/p&gt;

&lt;h2 id=&#34;rust-references&#34;&gt;Rust References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rustbyexample.com/index.html&#34;&gt;Rust by Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://doc.rust-lang.org/book/README.html&#34;&gt;The Rust Programming Language&lt;/a&gt; (free e-book)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to Blog in 2017</title>
      <link>https://michael-myers.github.io/blog/post/static-site-generators/</link>
      <pubDate>Fri, 03 Feb 2017 15:43:53 -0500</pubDate>
      
      <guid>https://michael-myers.github.io/blog/post/static-site-generators/</guid>
      <description>&lt;p&gt;My first blog, back in the early 2000s, was on a hosted blogging platform known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Blogger_(service)&#34;&gt;Blogger&lt;/a&gt;. It was simple and convenient: as the admin you just logged into the Blogger service, edited posts in your browser, and hit publish. This is basically how Tumblr still works today, although Tumblr&amp;rsquo;s innovation was to include media file hosting and allow everyone to repost each others&amp;rsquo; content.&lt;/p&gt;

&lt;p&gt;But Blogger content was &lt;em&gt;static&lt;/em&gt;, and textual. You could post a few paragraphs of text, and embed images if they were hosted elsewhere. Only later did Google buy out the service and integrate it with their photo-hosting service. In the mid-2000s, many geeks wanted more flexibility, like the ability to limit access to members only, integrate their own photo/video/audio collections, and ‚Äì most importantly ‚Äì control the appearance of their blog.&lt;/p&gt;

&lt;p&gt;So my second blog was generated with a Web Content Management System (CMS) and self-hosted on a home Windows XP PC running the &amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/LAMP_%28software_bundle%29#WAMP&#34;&gt;WAMP&lt;/a&gt;&amp;rdquo; software stack, with a DNS record from a free dynamic DNS service. If you&amp;rsquo;re a system admin or security expert you&amp;rsquo;re probably cringing. I am too. In hindsight, it&amp;rsquo;s a miracle if that PC was not 0wned by a hacker at some point, but at least I have no evidence to believe it was. But I thought my blog was pretty cool, it had a custom look, custom domain name, its own forums, file storage, a weather widget on the sidebar. I believe it was using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Drupal&#34;&gt;Drupal CMS&lt;/a&gt;. The 2000s saw this rise of the &amp;ldquo;web app,&amp;rdquo; a concept that an application was something that ran in a scripting language on a web server and presented you with a web page as the user interface. As a system programmer who thinks an application is a single self-contained compiled binary, I thought this was an anathema. But the rest of the tech world decided otherwise: websites that were not database-backed and server-side-scripted were totally 90s! That meant lame. 90s wasn&amp;rsquo;t cool again yet.&lt;/p&gt;

&lt;p&gt;The reason why the self-hosted CMS approach to blogging is cringey is that it is notoriously difficult to secure a CMS, especially one written in PHP. PHP is now known to be prone to reoccuring security issues because of flaws in its design (unvalidated input, access control problems, command injection issues, etc.), and the use of a SQL database means fighting a war agains SQL injection attacks from anyone who uses your site. Spammers will leave spam comments. You just want to run a blog, but now you&amp;rsquo;re a system admin for a web server, a database admin for a database, and you have to understand the PHP (or Java, or whatever) that generates your site on the fly every time a visitor loads a page. If you ever want to use a web hosting service for your CMS-based site instead of hosting it at home, you have to pay real money, because supporting and securing Apache, PHP, and MySQL is a full-time job! On top of all of that, all of this script and database stuff makes the site is slower to load, and prone to Denial of Service attacks.&lt;/p&gt;

&lt;p&gt;This is no way to live. And so, as is typical, the tech community decided that what is old is new again, and that static sites were actually a good idea that should never have been abandoned. Rolling my eyes so hard I went temporarily blind, I actually resisted even caring about the cool way to blog in the 2010s. I used LiveJournal for a bit. I tried a hosted Wordpress (Wordpress.com) account to blog about game console emulators. I got into using Tumblr, even though (or maybe &lt;em&gt;because&lt;/em&gt;) the tech community is not on there. But now I&amp;rsquo;ve decided to give a fresh look at what&amp;rsquo;s fresh, and give it a chance.&lt;/p&gt;

&lt;p&gt;Here are some things I noticed about the current Preferred Way for Cool Kids to Blog.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If you write any kind of code for a living, you host it on a free hosting service in the .io TLD. This is just what is fashionable, and like all fashion choices, it can&amp;rsquo;t really be explained. &amp;ldquo;Everyone is doing it&amp;rdquo;, including this blog. We are not all hosting sites in the British Indian Ocean Territory, but yes, this TLD exists because the UK &lt;a href=&#34;https://en.wikipedia.org/wiki/Depopulation_of_Chagossians_from_the_Chagos_Archipelago&#34;&gt;stole some Pacific Islanders&amp;rsquo; land&lt;/a&gt; during the Cold War, and its only other claim to fame might be &lt;a href=&#34;http://content.time.com/time/world/article/0,8599,1828469,00.html&#34;&gt;its black site CIA torture prison&lt;/a&gt;. How&amp;rsquo;s that for oblivious Silicon Valley tech privilege!&lt;/li&gt;
&lt;li&gt;Because HTML, JS, and CSS are nearly impossible to work in directly anymore (much like assembly code), people write their web page content in a highly simplified markup language, and then run &lt;em&gt;that&lt;/em&gt; through a compiler (oh, sorry, &lt;em&gt;static site generator&lt;/em&gt;) to produce a web site in actual HTML, JS, and CSS. The output is then posted to a web hosting service. There are some &lt;a href=&#34;https://staticsitegenerators.net&#34;&gt;450 static site generators&lt;/a&gt; to choose from. This site uses Hugo, which I&amp;rsquo;ll talk about in a future post. An even more popular choice is Jekyll, which is fine‚Ä¶for me to poop on.&lt;/li&gt;
&lt;li&gt;The simplified markup language of choice currently is &lt;a href=&#34;https://en.wikipedia.org/wiki/Markdown&#34;&gt;Markdown&lt;/a&gt;, which will also be the subject of a future post because it is pretty neat.&lt;/li&gt;
&lt;li&gt;Because supporting the ability for visitors to post comments would require a dynamic site, static sites have outsourced this responsibility to third-party services. That is, comments are implemented with an embedded JavaScript element that is loaded from a remote service. The dominant choice of service at the moment is &lt;a href=&#34;https://en.wikipedia.org/wiki/Disqus&#34;&gt;Disqus&lt;/a&gt;. This and any other user-account-based service that embeds its content on your blog is a privacy problem: it means Disqus is basically assigning you an identifier and following you around to all of the Disqus-enabled sites you visit. &lt;a href=&#34;https://www.ghostery.com&#34;&gt;Ghostery&lt;/a&gt; blocks Disqus by default, for this reason. I suggest using Twitter to reach me if you have a comment.&lt;/li&gt;
&lt;li&gt;Because static sites cannot track how many visitors they get and where they visited from, that too has been outsourced. &lt;a href=&#34;https://en.wikipedia.org/wiki/Google_Analytics&#34;&gt;Google Analytics&lt;/a&gt; is now more prevalent than HPV and herpes combined. I have had to delete it out of every web-related code repository that I have borrowed to make anything. Even if I&amp;rsquo;m the last one on Earth who cares about privacy, I will not be including that here. The same goes for social media sharing links. You&amp;rsquo;re a big boy and/or girl, I bet you&amp;rsquo;ll figure out how to share a URL yourself!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So there you have it, my take on the Way to Blog in the 2010s for Cool Kids. Thanks for reading. ‚Äì MM&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>